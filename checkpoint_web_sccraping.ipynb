{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4iaOS6tiMKkJjLN8tjwth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/modouseck/first-repo/blob/main/checkpoint_web_sccraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4jYlUtXgEp1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fb5baf-adc9-44d4-a164-ec2d52b41b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titre de l'article: Python (langage)\n",
            "\n",
            "Sections et paragraphes:\n",
            "\n",
            "== Introduction ==\n",
            "- Python(prononcé/pi.tɔ̃/) est unlangage de programmationinterprété,multiparadigmeetmultiplateformes. Il favorise laprogrammation impérativestructurée,fonctionnelleetorientée objet. Il est doté d'untypage dynamiquefort, d'une gestion automatique de lamémoireparramasse-mietteset d'unsystème de gestion d'exceptions; il est ainsi similaire àPerl,Ruby,Scheme,SmalltalketTcl.\n",
            "- Le langage Python est placé sous unelicence libreproche de lalicence BSDet fonctionne sur la plupart desplateformes informatiques, dessmartphonesauxordinateurs centraux, deWindowsàUnixavec notammentGNU/Linuxen passant parmacOS, ou encoreAndroid,iOS, et peut aussi être traduit enJavaou.NET. Il est conçu pour optimiser la productivité des programmeurs en offrant des outils dehaut niveauet unesyntaxesimple à utiliser.\n",
            "- Il est également apprécié par certainspédagoguesqui y trouvent unlangageoù la syntaxe, clairement séparée des mécanismes debas niveau, permet uneinitiationaisée aux concepts de base de la programmation[5]. Selon l'Index TIOBE, notamment en raison de son efficacité pour l'apprentissage automatique, sa popularité va croissante ; et en 2022 n'a toujours pas montré de signe de ralentissement[6].\n",
            "\n",
            "Nombre de liens internes trouvés: 485\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def get_html(url):\n",
        "    \"\"\"\n",
        "    Récupère et analyse le contenu HTML d'une page Wikipédia.\n",
        "    :param url:https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Accueil_principal\n",
        "    :return: BeautifulSoup object\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "def get_title(soup):\n",
        "    \"\"\"\n",
        "    Extrait le titre de l'article Wikipédia.\n",
        "    :param soup: BeautifulSoup object\n",
        "    :return: titre de l'article (str)\n",
        "    \"\"\"\n",
        "    title_tag = soup.find('h1', id='firstHeading')\n",
        "    return title_tag.get_text(strip=True) if title_tag else None\n",
        "\n",
        "\n",
        "def get_text_sections(soup):\n",
        "    \"\"\"\n",
        "    Extrait le texte de l'article organisé par sections et paragraphes.\n",
        "    :param soup: BeautifulSoup object\n",
        "    :return: dict mapping section titles to lists of paragraph textes\n",
        "    \"\"\"\n",
        "    content = soup.find('div', class_='mw-parser-output')\n",
        "    sections = {}\n",
        "    current_header = 'Introduction'\n",
        "    sections[current_header] = []\n",
        "\n",
        "    for elem in content.find_all(['h2', 'h3', 'p'], recursive=False):\n",
        "        if elem.name in ['h2', 'h3']:\n",
        "            # retirer l'ancre [modifier]\n",
        "            header_text = elem.get_text().replace('[modifier]', '').strip()\n",
        "            current_header = header_text\n",
        "            sections[current_header] = []\n",
        "        elif elem.name == 'p':\n",
        "            text = elem.get_text(strip=True)\n",
        "            if text:\n",
        "                sections[current_header].append(text)\n",
        "    return sections\n",
        "\n",
        "\n",
        "def get_internal_links(soup, base_url='https://fr.wikipedia.org'):\n",
        "    \"\"\"\n",
        "    Collecte tous les liens internes vers d'autres pages Wikipédia.\n",
        "    :param soup: BeautifulSoup object\n",
        "    :param base_url: domaine de base pour compléter les URLs relatives\n",
        "    :return: liste d'URLs internes\n",
        "    \"\"\"\n",
        "    links = set()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        # filtrer les liens vers d'autres articles\n",
        "        if href.startswith('/wiki/') and ':' not in href:\n",
        "            full_url = urljoin(base_url, href)\n",
        "            links.add(full_url)\n",
        "    return list(links)\n",
        "\n",
        "\n",
        "def parse_wikipedia(url):\n",
        "    \"\"\"\n",
        "    Fonction consolidée qui prend une URL Wikipédia et retourne:\n",
        "      - titre\n",
        "      - sections de texte\n",
        "      - liens internes\n",
        "    :param url: URL de la page Wikipédia\n",
        "    :return: dict contenant 'title', 'sections', 'links'\n",
        "    \"\"\"\n",
        "    soup = get_html(url)\n",
        "    title = get_title(soup)\n",
        "    sections = get_text_sections(soup)\n",
        "    links = get_internal_links(soup)\n",
        "    return {\n",
        "        'url': url,\n",
        "        'title': title,\n",
        "        'sections': sections,\n",
        "        'links': links\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Exemple de test sur la page Wikipédia 'Python (langage)'\n",
        "    test_url = 'https://fr.wikipedia.org/wiki/Python_(langage)'\n",
        "    result = parse_wikipedia(test_url)\n",
        "    print(f\"Titre de l'article: {result['title']}\\n\")\n",
        "    print(\"Sections et paragraphes:\")\n",
        "    for section, paras in result['sections'].items():\n",
        "        print(f\"\\n== {section} ==\")\n",
        "        for p in paras[:3]:  # n'afficher que les 3 premiers paragraphes de chaque section\n",
        "            print(f\"- {p}\")\n",
        "    print(f\"\\nNombre de liens internes trouvés: {len(result['links'])}\")\n"
      ]
    }
  ]
}